{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression models (both linear and non-linear) are used for predicting a real value, like salary for example. If your independent variable is time, then you are forecasting future values, otherwise your model is predicting present but unknown values. Regression technique vary from Linear Regression to SVR and Random Forests Regression.\n",
    "\n",
    "5 Methods of buiding models: i.e., remove unecessery features:\n",
    "1. all-in\n",
    "2. backward elimination\n",
    "3. forward selection\n",
    "4. bidirectional elimination\n",
    "5. score comparsion\n",
    "\n",
    "2-4 stepwise regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "support vector regression - nonlinear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid over-fitting, a regularization term can be introduced "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "decision tree intuition\n",
    "\n",
    "# random forest (combine different alogrihtm to find the best one, ensemble learning)\n",
    "1. pick at random k data points from the training set\n",
    "2. build hte decision tree associated to these k data points\n",
    "3. choose the number ntree of trees you want to build and repeat step 1 and 2\n",
    "4. for a new data point, make each of your ntree trees predict the value of Y to for the data point in question, and assign the new data point the average across all of the predicted Y values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. How can I improve each of these models ?\n",
    "\n",
    "In Part 10 - Model Selection, you will find the second section dedicated to Parameter Tuning, that will allow you to improve the performance of your models, by tuning them. You probably already noticed that each model is composed of two types of parameters:\n",
    "\n",
    "the parameters that are learnt, for example the coefficients in Linear Regression,\n",
    "the hyperparameters.\n",
    "The hyperparameters are the parameters that are not learnt and that are fixed values inside the model equations. For example, the regularization parameter lambda or the penalty parameter C are hyperparameters. So far we used the default value of these hyperparameters, and we haven't searched for their optimal value so that your model reaches even higher performance. Finding their optimal value is exactly what Parameter Tuning is about. So for those of you already interested in improving your model performance and doing some parameter tuning, feel free to jump directly to Part 10 - Model Selection.\n",
    "\n",
    "And as a BONUS, please find here some slides we made about Regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Classification models include linear models like Logistic Regression, SVM, and nonlinear ones like K-NN, Kernel SVM and Random Forests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I do not understand Support Vector Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN\n",
    "1. choose the number K of neighbors\n",
    "2. take the K nearest neighbors of the new data point, according to the Euclidean distance\n",
    "3. Among these K neighbors, count the number of data points in each category\n",
    "4. Assign the new data point to the category where you counted the most neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM)\n",
    "1. two points: support vector\n",
    "2. maximum margin hyperplane\n",
    "3. negative/positive hyperplane\n",
    "\n",
    "What's so popular about SVM\n",
    "\n",
    "The two support vectors are two points: 苹果里最像橘子的， 橘子里最像苹果的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kernel SVM\n",
    "\n",
    "linearly separable vs not linearly separable\n",
    "\n",
    "Map to not linerly separble into a higher-dimension space\n",
    "\n",
    "# Naive Bayes' Classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "Clustering is similar to classification, but the basis is different. In Clustering you don’t know what you are looking for, and you are trying to identify some segments or clusters in your data. When you use clustering algorithms on your dataset, unexpected things can suddenly pop up like structures, clusters and groupings you would have never thought of otherwise.\n",
    "\n",
    "## k-means++\n",
    "k-means++算法选择初始聚类中心的基本原则是：初始的聚类中心之间的相互距离要尽可能的远。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA原理\n",
    "https://zhuanlan.zhihu.com/p/21580949"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
